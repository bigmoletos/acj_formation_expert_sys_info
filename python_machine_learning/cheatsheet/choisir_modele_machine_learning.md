
# Cheatsheet : Choisir un Mod√®le de Machine Learning

Ce cheatsheet vous aide √† choisir un mod√®le de machine learning en fonction :
- **Du type de donn√©es (structur√©es/non structur√©es)**
- **De l'objectif (classification, r√©gression, clustering, r√©duction de dimensionnalit√©)**

---

## 1. **Crit√®re 1 : Quantit√© de Donn√©es**
- **Petites donn√©es (moins de 10 000 observations) :**
  - Pr√©f√©rer des mod√®les simples comme :
    - **R√©gression lin√©aire/logistique**
    - **K-Nearest Neighbors (KNN)**
    - **Decision Tree**
- **Donn√©es mod√©r√©es (10 000 - 100 000 observations) :**
  - Mod√®les plus complexes comme :
    - **Random Forest**
    - **Gradient Boosting (XGBoost, LightGBM, CatBoost)**
- **Donn√©es massives (+100 000 observations) :**
  - Mod√®les performants √† grande √©chelle :
    - **Descente de Gradient Stochastique (SGDClassifier, SGDRegressor)**
    - **R√©seaux de Neurones (Deep Learning)**

---

## 2. **Crit√®re 2 : Structure des Donn√©es**
- **Donn√©es structur√©es (tableau, features clairement d√©finies) :**
  - Utilisez des mod√®les bas√©s sur les arbres (**Random Forest, XGBoost**) ou des mod√®les lin√©aires (**R√©gression, SVM**).
- **Donn√©es non structur√©es (texte, images, audio) :**
  - **Texte :** NLP avec des mod√®les comme **TF-IDF + Logistic Regression**, ou des architectures modernes comme **Transformers (BERT, GPT)**.
  - **Images :** R√©seaux de Neurones Convolutionnels (**CNNs**).
  - **Audio :** Mod√®les d'analyse spectrale ou **RNN**.

---

## 3. **Crit√®re 3 : Objectif de l'Analyse**
### Classification (pr√©dire une classe)
- **Exemples de mod√®les :**
  - **Logistic Regression** (simple, efficace pour donn√©es lin√©aires)
  - **Random Forest / Gradient Boosting** (donn√©es complexes, robustesse)
  - **SVM** (donn√©es lin√©airement/non lin√©airement s√©parables)
- **Exemple de Dataset :**
  - [Iris Dataset (Scikit-learn)](https://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html)
    - Objectif : Classifier les fleurs en fonction de leur esp√®ce.

### R√©gression (pr√©dire une valeur continue)
- **Exemples de mod√®les :**
  - **R√©gression lin√©aire / Ridge / Lasso**
  - **Random Forest Regressor**
  - **Gradient Boosting Regressor (XGBoost, LightGBM)**
- **Exemple de Dataset :**
  - [Boston Housing Dataset (Seaborn)](https://seaborn.pydata.org/examples/many_facets.html)
    - Objectif : Pr√©dire le prix des maisons.

### Clustering (regrouper des observations similaires)
- **Exemples de mod√®les :**
  - **K-Means** (rapide, facile √† comprendre)
  - **DBSCAN** (robuste aux outliers)
  - **Gaussian Mixture Models (GMM)** (bas√© sur la probabilit√©)
- **Exemple de Dataset :**
  - [Planets Dataset (Seaborn)](https://seaborn.pydata.org/examples/index.html)
    - Objectif : Regrouper les exoplan√®tes selon leurs caract√©ristiques.

### R√©duction de Dimensionnalit√©
- **Exemples de mod√®les :**
  - **PCA (Principal Component Analysis)** (lin√©aire)
  - **t-SNE / UMAP** (non lin√©aires, pour visualisation)
- **Exemple de Dataset :**
  - [Digits Dataset (Scikit-learn)](https://scikit-learn.org/stable/auto_examples/classification/plot_digits_classification.html)
    - Objectif : R√©duire les dimensions pour visualiser des chiffres manuscrits.

---

## 4. **Crit√®re 4 : Nature des Variables**
- **Quantitatives (num√©riques) :**
  - Mod√®les bas√©s sur des moyennes ou des relations lin√©aires :
    - **R√©gression lin√©aire, Random Forest**
- **Qualitatives (cat√©goriques) :**
  - Mod√®les prenant en charge des variables cat√©gorielles :
    - **Gradient Boosting (CatBoost, LightGBM)**, **Logistic Regression**

---

## 5. **Ressources et Datasets**

### Scikit-learn Datasets :
- **Iris Dataset :** [Lien ici](https://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html)
- **Digits Dataset :** [Lien ici](https://scikit-learn.org/stable/auto_examples/classification/plot_digits_classification.html)
- **Boston Housing :** [Lien ici](https://scikit-learn.org/stable/datasets/toy_dataset.html#boston-dataset)

### Seaborn Datasets :
- **Planets Dataset :** [Lien ici](https://seaborn.pydata.org/examples/index.html)
- **Tips Dataset :** [Lien ici](https://seaborn.pydata.org/examples/index.html)

---

## 6. **Conseils G√©n√©raux**
1. **Toujours tester plusieurs mod√®les :**
   - Utilisez une biblioth√®que comme `scikit-learn` pour cr√©er des pipelines.
2. **Effectuer une validation crois√©e :**
   - Validez vos mod√®les avec **K-Fold Cross-Validation**.
3. **Hyperparam√®tres :**
   - Optimisez les hyperparam√®tres avec des outils comme **GridSearchCV** ou **RandomizedSearchCV**.
4. **Pr√©caution avec les donn√©es :**
   - V√©rifiez les outliers et les valeurs manquantes avant de choisir un mod√®le.

---

## 7. **Outils Recommand√©s**
- **Scikit-learn :** Pour mod√®les classiques.
- **Seaborn / Matplotlib :** Pour la visualisation.
- **TensorFlow / PyTorch :** Pour le Deep Learning.

---

Happy Modeling! üéØ
